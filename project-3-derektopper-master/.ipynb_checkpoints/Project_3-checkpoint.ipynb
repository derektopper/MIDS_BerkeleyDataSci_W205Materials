{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding User Behavior: Tracking Game Events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Project 3 by Derek Topper, for MIDS W205, 8/2/2020*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summary\n",
    "\n",
    "In this project, we are working as a data scientist for a gaming company interested in understanding the actions that players take in a game. In this project, we'll focus on different user purchases and their ability to join a guild. In this pipeline, we'll be able to stream data through our pipeline and complete queries on these items to better understand our data. Specifically, we'll deploy a docker cluster to demonstrate how events are captured by flask web server, how these events are published to the Kafka topic, the ingestion of the message and queries in Presto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Introduction\n",
    "\n",
    "In this project, I am a data scientist at a game development company. The company's latest mobile game has two events you're interested in tracking: buy a sword & join guild. Each has metadata characterstic of such events (i.e. sword amount, guild description, etc)\n",
    "\n",
    "##### Tasks\n",
    "\n",
    "- Instrument your API server to log events to Kafka\n",
    "\n",
    "- Assemble a data pipeline to catch these events: use Spark streaming to filter\n",
    "  select event types from Kafka, land them into HDFS/parquet to make them\n",
    "  available for analysis using Presto\n",
    "\n",
    "- Use Apache Bench to generate test data for your pipeline\n",
    "\n",
    "- Produce an analytics report where you provide a description of your pipeline\n",
    "  and some basic analysis of the events\n",
    "\n",
    "There are four files associated with this project:\n",
    "\n",
    "* This file is the report with step-by-step command annotations and the queries.\n",
    "* docker-compose.yml is the docker cluster configuration file.\n",
    "* game_api.py is the Python API for the Web Server.\n",
    "* write_stream.py is the Python spark scripts to separate, filter, and write the game event messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution and Exhibition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I started off by creating and navigating to my directory called w205/project-3-derektopper and copied the files used in my course's Week 14 materials. \n",
    "\n",
    "```\n",
    "cd w205/project-3-derektopper\n",
    "cp ~/w205/course-content/14-Patterns-for-Data-Pipelines/docker-compose.yml .\n",
    "cp ~/w205/course-content/14-Patterns-for-Data-Pipelines/game_api.py .\n",
    "cp ~/w205/course-content/14-Patterns-for-Data-Pipelines/*.py .\n",
    "```\n",
    "\n",
    "The docker-compose.yml file I used below contained the following docker containers. The container labeled mids will expose port 5000, which I will use for the game events web API. The Zookeeper container listens on port 32181, which is how Zookeeper connects to Kafka. Kafka, alternatively, listens for data streaming on port 29092. The Spark container allows us \n",
    "\n",
    "The docker-compose.yml has various docker containers within it. The mids container exposes port 5000, which will be used for game events Web API. Zookeeper listens on port number 32181, which Kafka uses to connect to Zookeeper. Kafka listens on port number 29092 for data streaming. The Presto container will allow us to query this data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the docker-compose.yml file.\n",
    "\n",
    "```\n",
    "---\n",
    "version: '2'\n",
    "services:\n",
    "  zookeeper:\n",
    "    image: confluentinc/cp-zookeeper:latest\n",
    "    environment:\n",
    "      ZOOKEEPER_CLIENT_PORT: 32181\n",
    "      ZOOKEEPER_TICK_TIME: 2000\n",
    "    expose:\n",
    "      - \"2181\"\n",
    "      - \"2888\"\n",
    "      - \"32181\"\n",
    "      - \"3888\"\n",
    "    extra_hosts:\n",
    "      - \"moby:127.0.0.1\"\n",
    "\n",
    "  kafka:\n",
    "    image: confluentinc/cp-kafka:latest\n",
    "    depends_on:\n",
    "      - zookeeper\n",
    "    environment:\n",
    "      KAFKA_BROKER_ID: 1\n",
    "      KAFKA_ZOOKEEPER_CONNECT: zookeeper:32181\n",
    "      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092\n",
    "      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n",
    "    expose:\n",
    "      - \"9092\"\n",
    "      - \"29092\"\n",
    "    extra_hosts:\n",
    "      - \"moby:127.0.0.1\"\n",
    "\n",
    "  cloudera:\n",
    "    image: midsw205/hadoop:0.0.2\n",
    "    hostname: cloudera\n",
    "    expose:\n",
    "      - \"8020\" # nn\n",
    "      - \"8888\" # hue\n",
    "      - \"9083\" # hive thrift\n",
    "      - \"10000\" # hive jdbc\n",
    "      - \"50070\" # nn http\n",
    "    ports:\n",
    "      - \"8888:8888\"\n",
    "    extra_hosts:\n",
    "      - \"moby:127.0.0.1\"\n",
    "\n",
    "  spark:\n",
    "    image: midsw205/spark-python:0.0.6\n",
    "    stdin_open: true\n",
    "    tty: true\n",
    "    volumes:\n",
    "      - ~/w205:/w205\n",
    "    expose:\n",
    "      - \"8888\"\n",
    "    ports:\n",
    "      - \"8889:8888\" # 8888 conflicts with hue\n",
    "    depends_on:\n",
    "      - cloudera\n",
    "    environment:\n",
    "      HADOOP_NAMENODE: cloudera\n",
    "      HIVE_THRIFTSERVER: cloudera:9083\n",
    "    extra_hosts:\n",
    "      - \"moby:127.0.0.1\"\n",
    "    command: bash\n",
    "\n",
    "  presto:\n",
    "    image: midsw205/presto:0.0.1\n",
    "    hostname: presto\n",
    "    volumes:\n",
    "      - ~/w205:/w205\n",
    "    expose:\n",
    "      - \"8080\"\n",
    "    environment:\n",
    "      HIVE_THRIFTSERVER: cloudera:9083\n",
    "    extra_hosts:\n",
    "      - \"moby:127.0.0.1\"\n",
    "\n",
    "  mids:\n",
    "    image: midsw205/base:0.1.9\n",
    "    stdin_open: true\n",
    "    tty: true\n",
    "    volumes:\n",
    "      - ~/w205:/w205\n",
    "    expose:\n",
    "      - \"5000\"\n",
    "    ports:\n",
    "      - \"5000:5000\"\n",
    "    extra_hosts:\n",
    "      - \"moby:127.0.0.1\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I wanted to edit the game_api to increase the functionality offered within our game. As a result, I added various events to the game_api.py file. These events will allow us to track new user purchases. Whereas, this file initially contained sword purchasing, I added the ability to purchase armor, a knife, bow and arrows. I also included the ability to join a guild. Thus, I used the following Python code, and web API calls to log the events to kafka and allowed additional parameters in these calls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`vim game_api.py`\n",
    "\n",
    "The new `game_api.py` file:\n",
    "\n",
    "```\n",
    "#!/usr/bin/env python\n",
    "import json\n",
    "from kafka import KafkaProducer\n",
    "from flask import Flask, request\n",
    "\n",
    "app = Flask(__name__)\n",
    "producer = KafkaProducer(bootstrap_servers='kafka:29092')\n",
    "\n",
    "\n",
    "def log_to_kafka(topic, event):\n",
    "    event.update(request.headers)\n",
    "    producer.send(topic, json.dumps(event).encode())\n",
    "\n",
    "\n",
    "@app.route(\"/\")\n",
    "def default_response():\n",
    "    default_event = {'event_type': 'default'}\n",
    "    log_to_kafka('events', default_event)\n",
    "    return \"This is the default response!\\n\"\n",
    "\n",
    "\n",
    "@app.route(\"/purchase_a_sword\")\n",
    "def purchase_a_sword():\n",
    "    purchase_sword_event = {'event_type': 'purchase_sword',\n",
    "                           'description': 'upgrade sword strength',\n",
    "                           'amount': '1'}\n",
    "    log_to_kafka('events', purchase_sword_event)\n",
    "    return \"Sword Purchased!\\n\"\n",
    "\n",
    "@app.route(\"/purchase_a_knife\")\n",
    "def purchase_a_knife():\n",
    "    purchase_knife_event = {'event_type': 'purchase_knife',\n",
    "                           'description': 'upgrade knife strength',\n",
    "                           'amount': '1'}\n",
    "    log_to_kafka('events', purchase_knife_event)\n",
    "    return \"Knife Purchased!\\n\"\n",
    "\n",
    "@app.route(\"/purchase_armor\")\n",
    "def purchase_armor():\n",
    "    purchase_armor_event = {'event_type': 'purchase_armor',\n",
    "                           'description': 'upgrade armor strength',\n",
    "                           'amount': '1'}\n",
    "    log_to_kafka('events', purchase_armor_event)\n",
    "    return \"Armor Purchased!\\n\"\n",
    "\n",
    "@app.route(\"/purchase_bow\")\n",
    "def purchase_bow():\n",
    "    purchase_bow_event = {'event_type': 'purchase_bow',\n",
    "                           'description': 'upgrade bow strength',\n",
    "                           'amount': '1'}\n",
    "    log_to_kafka('events', purchase_bow_event)\n",
    "    return \"Bow Purchased!\\n\"\n",
    "\n",
    "@app.route(\"/purchase_arrow\")\n",
    "def purchase_arrow():\n",
    "    purchase_arrow_event = {'event_type': 'purchase_arrow',\n",
    "                           'description': 'upgrade arrow strength',\n",
    "                           'amount': '1'}\n",
    "    log_to_kafka('events', purchase_arrow_event)\n",
    "    return \"Arrow Purchased!\\n\"\n",
    "\n",
    "\n",
    "@app.route(\"/join_guild\")\n",
    "def join_guild():\n",
    "    join_guild_event = {'event_type': 'join_guild',\n",
    "                        'description': 'joining a guild'}\n",
    "    log_to_kafka('events', join_guild_event)\n",
    "    return \"Joined Guild!\\n\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next in order to be able to work with this data, I used PySpark to batch the contents from kafka and write them to hdfs. We were able to keep the extracted events whole, but was able to create two separate parquet files, one for all of the purchases that users made, such as buying a sword, and one for the guild membership, such as a user joining a guild. This allows us to do everything in a stream.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `write_stream.py` file now:\n",
    "\n",
    "```\n",
    "#!/usr/bin/env python\n",
    "\"\"\"Extract events from kafka and write them to hdfs\n",
    "\"\"\"\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "def transaction_event_schema():\n",
    "    \"\"\"\n",
    "    root\n",
    "    |-- Accept: string (nullable = true)\n",
    "    |-- Host: string (nullable = true)\n",
    "    |-- User-Agent: string (nullable = true)\n",
    "    |-- event_type: string (nullable = true)\n",
    "    |-- description: string (nullable = true)\n",
    "    |-- amount: string (nullable = true)\n",
    "    |-- timestamp: string (nullable = true)\n",
    "    \"\"\"\n",
    "    return StructType([\n",
    "        StructField(\"Accept\", StringType(), True),\n",
    "        StructField(\"Host\", StringType(), True),\n",
    "        StructField(\"User-Agent\", StringType(), True),\n",
    "        StructField(\"event_type\", StringType(), True),\n",
    "        StructField(\"amount\", StringType(), True),\n",
    "        StructField(\"description\", StringType(), True),\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "def join_guild_event_schema():\n",
    "    \"\"\"\n",
    "    root\n",
    "    |-- Accept: string (nullable = true)\n",
    "    |-- Host: string (nullable = true)\n",
    "    |-- User-Agent: string (nullable = true)\n",
    "    |-- event_type: string (nullable = true)\n",
    "    |-- timestamp: string (nullable = true)\n",
    "    \"\"\"\n",
    "    return StructType([\n",
    "        StructField(\"Accept\", StringType(), True),\n",
    "        StructField(\"Host\", StringType(), True),\n",
    "        StructField(\"User-Agent\", StringType(), True),\n",
    "        StructField(\"event_type\", StringType(), True),\n",
    "    ])\n",
    "\n",
    "@udf('boolean')\n",
    "def is_purchase(event_as_json):\n",
    "    \"\"\"udf for filtering events\n",
    "    \"\"\"\n",
    "    event = json.loads(event_as_json)\n",
    "    if 'purchase' in event['event_type']:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "@udf('boolean')\n",
    "def is_join_guild(event_as_json):\n",
    "    \"\"\"udf for filtering events\n",
    "    \"\"\"\n",
    "    event = json.loads(event_as_json)\n",
    "    if event['event_type'] == 'join_guild':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"main\n",
    "    \"\"\"\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"ExtractEventsJob\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    raw_events = spark \\\n",
    "        .readStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "        .option(\"subscribe\", \"events\") \\\n",
    "        .load()\n",
    "\n",
    "    purchases = raw_events \\\n",
    "        .filter(is_purchase(raw_events.value.cast('string'))) \\\n",
    "        .select(raw_events.value.cast('string').alias('raw_event'),\n",
    "                raw_events.timestamp.cast('string'),\n",
    "                from_json(raw_events.value.cast('string'),\n",
    "                          transaction_event_schema()).alias('json')) \\\n",
    "        .select('raw_event', 'timestamp', 'json.*')\n",
    "    \n",
    "    guild_membership = raw_events \\\n",
    "        .filter(is_join_guild(raw_events.value.cast('string'))) \\\n",
    "        .select(raw_events.value.cast('string').alias('raw_event'),\n",
    "                raw_events.timestamp.cast('string'),\n",
    "                from_json(raw_events.value.cast('string'),\n",
    "                          join_guild_event_schema()).alias('json')) \\\n",
    "        .select('raw_event', 'timestamp', 'json.*')\n",
    "\n",
    "    sink_purchases = purchases \\\n",
    "        .writeStream \\\n",
    "        .format(\"parquet\") \\\n",
    "        .option(\"checkpointLocation\", \"/tmp/checkpoints_for_purchases\") \\\n",
    "        .option(\"path\", \"/tmp/purchases\") \\\n",
    "        .trigger(processingTime=\"120 seconds\") \\\n",
    "        .start()\n",
    "    \n",
    "    sink_guild_membership = guild_membership \\\n",
    "        .writeStream \\\n",
    "        .format(\"parquet\") \\\n",
    "        .option(\"checkpointLocation\", \"/tmp/checkpoints_for_guild_membership\") \\\n",
    "        .option(\"path\", \"/tmp/guild_membership\") \\\n",
    "        .trigger(processingTime=\"120 seconds\") \\\n",
    "        .start()\n",
    "\n",
    "    sink_purchases.awaitTermination()\n",
    "    sink_guild_membership.awaitTermination()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Docker\n",
    "\n",
    "Next I want to spin up the docker container cluster. The docker containers included in the cluster are kafka, zookeeper, presto, spark and mids. I first ensure that there are no stray containers and that all the networks are used by at least by one container. Then I spin up the docker container cluster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `docker ps -a`\n",
    "* `docker network ls`\n",
    "* `docker network prune`\n",
    "* `docker-compose up -d`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I created a Kafka topic called events.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```docker-compose exec kafka kafka-topics --create --topic events --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I wanted to be able to run Flask, using the game_api.py I created so that I could generate game events. Thus, I used a command line interface window to run the following command. This allows me to launch a flask web server listening on port 5000 from any hosts, specifically, 0.0.0.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```docker-compose exec mids env FLASK_APP=/w205/project-3-derektopper/game_api.py flask run --host 0.0.0.0```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that all of our messages that we'll publish into kafka are monitored, I continuously ran kafkacat in a new terminal to ensure this data is collected.\n",
    "\n",
    "`docker-compose exec mids kafkacat -C -b kafka:29092 -t events -o beginning`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we execute the `write_stream.py` file, in a new terminal, to write purchase events using Spark. We then run the streaming job that we want to link up using the following command.\n",
    "\n",
    "```docker-compose exec spark spark-submit /w205/project-3-derektopper/write_stream.py```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, in a new terminal, I generated test data for our pipeline with Apache Bench. The following commands represent test data for 3 users (user1.comcast.com, user2.att.com and user3.spectrum.com) generating 5, 10, or 15 of each event (default, purchase_a_sword, purchase_a_knife, join_guild, purchase_armor, leave_guild, attack_enemy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `docker-compose exec mids ab -n 10 -H \"Host: user1.comcast.com\" http://localhost:5000/`\n",
    "* `docker-compose exec mids ab -n 10 -H \"Host: user2.att.com\" http://localhost:5000/`\n",
    "* `docker-compose exec mids ab -n 10 -H \"Host: user3.spectrum.com\" http://localhost:5000/`\n",
    "\n",
    "* `docker-compose exec mids ab -n 10 -H \"Host: user1.comcast.com\" http://localhost:5000/purchase_a_sword`\n",
    "* `docker-compose exec mids ab -n 15 -H \"Host: user2.att.com\" http://localhost:5000/purchase_a_sword`\n",
    "* `docker-compose exec mids ab -n 15 -H \"Host: user3.spectrum.com\" http://localhost:5000/purchase_a_sword`\n",
    "\n",
    "* `docker-compose exec mids ab -n 10 -H \"Host: user1.comcast.com\" http://localhost:5000/purchase_a_knife`\n",
    "* `docker-compose exec mids ab -n 10 -H \"Host: user2.att.com\" http://localhost:5000/purchase_a_knife`\n",
    "* `docker-compose exec mids ab -n 10 -H \"Host: user3.spectrum.com\" http://localhost:5000/purchase_a_knife`\n",
    "\n",
    "* `docker-compose exec mids ab -n 10 -H \"Host: user1.comcast.com\" http://localhost:5000/join_a_guild`\n",
    "* `docker-compose exec mids ab -n 10 -H \"Host: user2.att.com\" http://localhost:5000/join_a_guild`\n",
    "* `docker-compose exec mids ab -n 5 -H \"Host: user3.spectrum.com\" http://localhost:5000/join_a_guild`\n",
    "\n",
    "* `docker-compose exec mids ab -n 15 -H \"Host: user1.comcast.com\" http://localhost:5000/purchase_armor`\n",
    "* `docker-compose exec mids ab -n 10 -H \"Host: user2.att.com\" http://localhost:5000/purchase_armor`\n",
    "* `docker-compose exec mids ab -n 10 -H \"Host: user3.spectrum.com\" http://localhost:5000/purchase_armor`\n",
    "\n",
    "* `docker-compose exec mids ab -n 10 -H \"Host: user1.comcast.com\" http://localhost:5000/purchase_arrow`\n",
    "* `docker-compose exec mids ab -n 15 -H \"Host: user2.att.com\" http://localhost:5000/purchase_bow`\n",
    "* `docker-compose exec mids ab -n 5 -H \"Host: user3.spectrum.com\" http://localhost:5000/purchase_arrow`\n",
    "\n",
    "* `docker-compose exec mids ab -n 5 -H \"Host: user1.comcast.com\" http://localhost:5000/purchase_bow`\n",
    "* `docker-compose exec mids ab -n 10 -H \"Host: user2.att.com\" http://localhost:5000/purchase_arrow`\n",
    "* `docker-compose exec mids ab -n 15 -H \"Host: user3.spectrum.com\" http://localhost:5000/purchase_bow`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, in a new terminal, we open hive in our cloudera container. We'll then use hive to create each of the external tables that we'll want to query. Now we've created two tables, one for all our purchases and the other for our guild membership."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`docker-compose exec cloudera hive`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`create external table if not exists default.purchases (Accept string, Host string, User_Agent string, event_type string, timestamp string, raw_event string, amount string) stored as parquet location '/tmp/purchases'  tblproperties (\"parquet.compress\"=\"SNAPPY\");`\n",
    "\n",
    "`create external table if not exists default.guild_membership (Accept string, Host string, User_Agent string, event_type string, timestamp string, raw_event string) stored as parquet location '/tmp/guild_membership'  tblproperties (\"parquet.compress\"=\"SNAPPY\");`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Queries\n",
    "\n",
    "Then, we're going to use Presto to run queries to check that the datastream is being saved.\n",
    "\n",
    "docker-compose exec presto presto --server presto:8080 --catalog hive --schema default\n",
    "\n",
    "(in Presto)\n",
    "\n",
    "show tables;\n",
    "describe purchases;\n",
    "select * from purchases;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we write commands in Presto. The first one, `show tables`, allows us to see the two tables we created in the earlier hive step.\n",
    "\n",
    "`presto:default> show tables;`\n",
    "\n",
    "```\n",
    "      Table       \n",
    "------------------\n",
    " guild_membership \n",
    " purchases        \n",
    "(2 rows)\n",
    "```\n",
    "\n",
    "Then we can look at the command `describe purchases` then we can see the various columns and types for the table's variables. There's one column for each piece of data we have.\n",
    "\n",
    "`presto:default> describe purchases;`\n",
    "\n",
    "```\n",
    "   Column   |  Type   | Comment \n",
    "------------+---------+---------\n",
    " accept     | varchar |         \n",
    " host       | varchar |         \n",
    " user_agent | varchar |         \n",
    " event_type | varchar |         \n",
    " timestamp  | varchar |         \n",
    " raw_event  | varchar |         \n",
    " amount     | varchar |         \n",
    "(7 rows)\n",
    "``` \n",
    "\n",
    "Next we look at how many rows are in our purchase data. I added 165 entries to the purchases table in our data and thus there 165 different item purchases.\n",
    "\n",
    "`presto:default> select count(amount) from purchases;`\n",
    "\n",
    "```\n",
    " _col0 \n",
    "-------\n",
    "   165 \n",
    "(1 row)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the guild_membership table we made, and we can see that this table only has 6 columns, with there not being an amount column in the guild membership table.\n",
    "\n",
    "`presto:default> describe guild_membership;`\n",
    "\n",
    "```   Column   |  Type   | Comment \n",
    "------------+---------+---------\n",
    " accept     | varchar |         \n",
    " host       | varchar |         \n",
    " user_agent | varchar |         \n",
    " event_type | varchar |         \n",
    " timestamp  | varchar |         \n",
    " raw_event  | varchar |         \n",
    "(6 rows)\n",
    "```\n",
    "\n",
    "By analyzing this data, we can also see that there are just 25 rows in the guild_membership table.\n",
    "\n",
    "\n",
    "`presto:default> select count(*) from guild_membership;`\n",
    "\n",
    "```_col0 \n",
    "-------\n",
    "    25 \n",
    "(1 row)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can use this data to answer some of the questions we have about the data. For example, we might want to know how many different purchases each user made to see which user made the most. For example, we might want to see this information to see how invested in our game a user is. Thus, we can run a command that allows us to group each user by their number of purchases and see how many purchases they made. For example, in this table, we can see that `user2.att.com` made the most purchases, with 60, while `user1.comcast.com` made the least purchases with just 50. \n",
    "\n",
    "`presto:default> select host, count(*) from purchases group by host;`\n",
    "\n",
    "```   host      | _col1 \n",
    "--------------------+-------\n",
    " user3.spectrum.com |    55 \n",
    " user1.comcast.com  |    50 \n",
    " user2.att.com      |    60 \n",
    "(3 rows)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we may want to analyze user behavior to see what items are purchased the most often. If we have this information, we can better understand how our users interact with the game's options and see what mechanics they use most often. For example, in this case, we can see that arrow purchses are the least common, with just 25, while sword purchases are much more common with 40. Using this table, we can also see things like that our users bought more swords than knives, which could potentially tell our company that we should focus on future expansions around the sword mechanic of the game, such as by offering different sword customization options or allowing users to purchase sword sheaths, rather than those centered around knives. \n",
    "\n",
    "`presto:default> select event_type , count(amount) as count from purchases group by event_type  order by count;`\n",
    "\n",
    "```event_type     | count \n",
    "----------------+-------\n",
    " purchase_arrow |    25 \n",
    " purchase_knife |    30 \n",
    " purchase_bow   |    35 \n",
    " purchase_armor |    35 \n",
    " purchase_sword |    40 \n",
    "(5 rows)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may also want to evaluate an individual user. In this case, we look at the user history for `user3.spectrum.com`. We can see that they purchased many more bows than they did arrows. As one would expect that more arrows are purchased, this could indicate some user behavior that we may want to evaluate more deeply.\n",
    "\n",
    "`presto:default> select event_type , count(*) as count from purchases where Host = 'user3.spectrum.com' group by event_type  order by count;`\n",
    "\n",
    "```\n",
    "    event_type    | count \n",
    "----------------+-------\n",
    " purchase_arrow |     5 \n",
    " purchase_knife |    10 \n",
    " purchase_armor |    10 \n",
    " purchase_sword |    15 \n",
    " purchase_bow   |    15 \n",
    "(5 rows)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also evaluate the number of different purchases that are made of a specific type by users. Let's say we want to know the breakdown of user purchases of a specific item. We are able to do this, such as the breakdown of bow and arrow purchases as shown below. This type of information could be useful to potentially offer different discounts or deals based on users who purchase a certain amount of multiple types of items.\n",
    "\n",
    "`presto:default> select event_type, count(amount) as count from purchases where raw_event = 'purchase_bow' group by event_type order by count;`\n",
    "```\n",
    "     event_type     | count \n",
    "--------------------+-------\n",
    " user1.comcast.com  |     5 \n",
    " user2.att.com      |    15 \n",
    " user3.spectrum.com |    15 \n",
    "(3 rows)\n",
    "```\n",
    "\n",
    "`presto:default> select event_type, count(amount) as count from purchases where raw_event = 'purchase_arrow' group by event_type order by count;`\n",
    "```\n",
    "event_type     | count \n",
    "--------------------+-------\n",
    " user3.spectrum.com |     5 \n",
    " user1.comcast.com  |    10 \n",
    " user2.att.com      |    10 \n",
    "(3 rows)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can evaluate data related to the guilds. We have various data about guild membership requests. Thus, we are able to use our data to see which users sent join guild requests. If we look below, we can see that every user requested to join a guild. While obviously this 100% join rate is related to our lack of users, this can be helpful for knowing the ultization rate of guilds and allow us to make game decisions as a result.\n",
    "\n",
    "`presto:default> select distinct(event_type) from guild_membership group by event_type;`\n",
    "\n",
    "```\n",
    "     event_type     \n",
    "--------------------\n",
    " user1.comcast.com  \n",
    " user2.att.com      \n",
    " user3.spectrum.com \n",
    "(3 rows)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we want to take down the docker cluster and confirm that it was taken down properly. \n",
    "\n",
    "`docker-compose down`\n",
    "\n",
    "`docker-compose ps`\n",
    "\n",
    "`docker ps -a`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, in summary, this assignment saw me successfully create a docker cluster. I showed that multiple game events could be captured by a web-based server and log information to publish to kafka.  I used Spark streaming to filter selected event types from Kafka, and converted them into parquet to make them available for analysis using Presto. I used Apache Bench to generate web server API calls to generate test data for my pipeline. I ran various Presto queries and produced an analytics report to provide a description of the pipeline and some analysis of the events.\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
