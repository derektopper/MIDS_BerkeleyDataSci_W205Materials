{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derek Topper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file shows which commands were used to create the data pipeline to ultimately run queries on the assessment data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Navigate to folder I want to work in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cd w205/project-2-derektopper`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting assignment data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`curl -L -o assessment-attempts-20180128-121051-nested.json https://goo.gl/ME6hjp`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get Docker Compose File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cp ~/w205/course-content/08-Querying-Data/docker-compose.yml .`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Spin up detached docker container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`docker-compose up -d`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Look at kafka logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`docker-compose logs -f kafka`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating a kafka topic called assessments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`docker-compose exec kafka kafka-topics --create --topic assessments --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### View description of kafka topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`docker-compose exec kafka kafka-topics --describe --topic assessments --zookeeper zookeeper:32181`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using jq to examine information in the json file (bash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`docker-compose exec mids bash -c \"cat /w205/project-2-derektopper/assessment-attempts-20180128-121051-nested.json\"`\n",
    "\n",
    "`docker-compose exec mids bash -c \"cat /w205/project-2-derektopper/assessment-attempts-20180128-121051-nested.json | jq '.'\"`\n",
    "\n",
    "\n",
    "`docker-compose exec mids bash -c \"cat /w205/project-2-derektopper/assessment-attempts-20180128-121051-nested.json | jq '.[]' -c\"`\n",
    "\n",
    "`docker-compose exec mids bash -c \"cat /w205/project-2-derektopper/assessment-attempts-20180128-121051-nested.json | jq '.[]'\"`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use kafkacat in producer mode to read messages (with printed message to ensure no errors). Consume messages and prints word count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`docker-compose exec mids bash -c \"cat /w205/project-2-derektopper/assessment-attempts-20180128-121051-nested.json | jq '.[]' -c | kafkacat -P -b kafka:29092 -t assessments && echo 'Produced messages.'\"`\n",
    "\n",
    "`docker-compose exec mids bash -c \"kafkacat -C -b kafka:29092 -t assessments -o beginning -e\"`\n",
    "\n",
    "`docker-compose exec mids bash -c \"kafkacat -C -b kafka:29092 -t assessments -o beginning -e\" | wc -l`\n",
    "\n",
    "Output: 3281"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run spark using spark container "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`docker-compose exec spark pyspark`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### read stuff from kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`messages = spark.read.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"kafka:29092\").option(\"subscribe\",\"assessments\").option(\"startingOffsets\", \"earliest\").option(\"endingOffsets\", \"latest\").load()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### see the schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`messages.printSchema()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output:\n",
    "\n",
    "root\n",
    "* |-- key: binary (nullable = true)\n",
    "* |-- value: binary (nullable = true)\n",
    "* |-- topic: string (nullable = true)\n",
    "* |-- partition: integer (nullable = true)\n",
    "* |-- offset: long (nullable = true)\n",
    "* |-- timestamp: timestamp (nullable = true)\n",
    "* |-- timestampType: integer (nullable = true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### see the messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`messages.show()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### cache messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`messages.cache()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cast messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`messages_as_strings=messages.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### take a look at stringed messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`messages_as_strings.show()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### take a look at stringed schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`messages_as_strings.printSchema()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### take a look at count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`messages_as_strings.count()`\n",
    "* 3280\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Unroll data and view first entry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`messages_as_strings.select('value').take(1)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### View entry and extract the value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`messages_as_strings.select('value').take(1)[0].value`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now work with json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`import json`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Unroll data and view first entry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`assessment=json.loads(messages_as_strings.select('value').take(1)[0].value)`\n",
    "\n",
    "`assessment`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### print an item from this assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`print(assessment['exam_name'])`\n",
    "* Normal Forms and All That Jazz Master Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### write stringed assessments data to hdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`messages_as_strings.write.parquet(\"/tmp/messages_as_strings\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check out results from another window\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`docker-compose exec cloudera hadoop fs -ls /tmp/`\n",
    "\n",
    "`docker-compose exec cloudera hadoop fs -ls /tmp/messages_as_strings/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### view data back in spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`messages_as_strings.show()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### fix unicode data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`import sys`\n",
    "\n",
    "`sys.stdout = open(sys.stdout.fileno(), mode='w', encoding='utf8', buffering=1)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### load json, using RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`import json`\n",
    "\n",
    "`messages_as_strings.rdd.map(lambda x: json.loads(x.value)).toDF().show()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### unroll and save extracted data with json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`extracted = messages_as_strings.rdd.map(lambda x: json.loads(x.value)).toDF()`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### view data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`extracted.show()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### view schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`extracted.printSchema()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save as a parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`extracted.write.parquet(\"/tmp/extracted\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### check out extracted file (in other window)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`docker-compose exec cloudera hadoop fs -ls /tmp/extracted/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`extracted.cache()`\n",
    "`extracted.registerTempTable('extracted')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the commands above, I processed this data so it could be queried using the code below.\n",
    "\n",
    "To accomplish this, I used the following:\n",
    "* Docker, a platform which provides an environment to work in using containers\n",
    "* Kafka, a platform that takes a topic, which I named assessments because the data contained is information about various assessments, and takes in the data in that topic to allow us to build a streaming pipeline.\n",
    "* Spark, a big data processing tool that allows us to extract the Kafka data as a JSON and work with it\n",
    "* Hadoop, a big data platform that stores our data in a common and useable format to be used by other data scientists.\n",
    "\n",
    "Upon working with this data, I was able to convert it into a JSON format, which meant that the file contained nested dictionaries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Queries \n",
    "\n",
    "Now that the data is in a useable format, I am able to create different queries to answer some of the questions posed in the assignment.\n",
    "\n",
    "I am making a couple of key assumptions worth noting. Firstly, I am assuming that each row represents a unique assessment and that the data has no duplicated values. Since there was not a defined data dictionary, I am unable to determine whether there are duplicated attempts. For example, I noticed that there are instances where both keen_id and user_exam_id appear  in multiple rows, but I cannot say that we should remove certain entries, without a data dictionary, as both of those IDs could be a row ID or a test taker ID, and I believe that is not covered by this project.\n",
    "\n",
    "Additionally, as a result of the lack of a data dictionary, I will mention what I interpret each variable to represent below.\n",
    "\n",
    "In trying to determine some of the business questions that might be important, I tried to consider what types of things someone looking at this data would want to know. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, I wanted to look at how many assessments are in the dataset? This could simply be done by counting each row, as we are assuming there aren't duplicated entries. This means that our analysis had 3,280 instances that we could examine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`spark.sql('select count(keen_id) from extracted limit 10').show()`\n",
    "\n",
    "`+--------------+\n",
    "|count(keen_id)|\n",
    "+--------------+\n",
    "|          3280|\n",
    "+--------------+`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then wanted to look at what the most common and least common courses taken were. This is an important value as it could help an analyst see what the breakdown of assessments looks like and where resources were allocated in the data. It can allow us to see that one course is much more popular than another course. \n",
    "\n",
    "Notably, I chose to use the exam_name variable, as this appeared to be the name of a specific course that an assessment came from. I am also assuming that the number of assessments and the number of courses are taken are the same.\n",
    "\n",
    "Thus looking at the data below, we can see that the most popular courses are:\n",
    "* Learning Git (394 Students)\n",
    "* Introduction to Python (162 Students)\n",
    "* Introduction to Java 8 (158 Students)\n",
    "\n",
    "Additionally, the least common courses are:\n",
    "* Nulls, Three-valued Logic and Missing Information (1 Student)\n",
    "* Native Web Apps for Android (1 Student)\n",
    "* Learning to Visualize Data with D3.js (1 Student)\n",
    "* Operating Red Hat Enterprise Linux Servers (1 Student)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Most Common`\n",
    "\n",
    "`spark.sql(\"select exam_name, count(exam_name)  from extracted group by exam_name order by count(exam_name) desc\").show(3)`\n",
    "\n",
    "`+--------------------+----------------+ \n",
    "|           exam_name|count(exam_name)|\n",
    "+--------------------+----------------+\n",
    "|        Learning Git|             394|\n",
    "|Introduction to P...|             162|\n",
    "|Introduction to J...|             158|\n",
    "+--------------------+----------------+`\n",
    "\n",
    "`Least Common`\n",
    "\n",
    "`spark.sql(\"select exam_name, count(exam_name)  from extracted group by exam_name order by count(exam_name) \").show(5)\n",
    "`\n",
    "`+--------------------+----------------+     \n",
    "|           exam_name|count(exam_name)|\n",
    "+--------------------+----------------+\n",
    "|Native Web Apps f...|               1|\n",
    "|Learning to Visua...|               1|\n",
    "|Nulls, Three-valu...|               1|\n",
    "|Operating Red Hat...|               1|\n",
    "|Learning Spring P...|               2|\n",
    "+--------------------+----------------+\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to answer a question like, did more people took Introduction to Machine Learning or Advanced Machine Learning, then we can do something like that using the code below. This can be useful if we want to compare two classes. In this case, we can assume the Introduction to Machine Learning is a class that would be taken before a class like Advanced Machine Learning, which could help explain why the introductory, easier course had 119 students take it, while the advanced, harder course had 67 students take it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`spark.sql(\"select exam_name as course, count(*) as num_takers from extracted where (exam_name like 'Introduction to Machine Learning' or exam_name like 'Advanced Machine Learning') group by exam_name\").show() `\n",
    "\n",
    "`+--------------------+----------+\n",
    "|              course|num_takers|\n",
    "+--------------------+----------+\n",
    "|Introduction to M...|       119|\n",
    "|Advanced Machine ...|        67|\n",
    "+--------------------+----------+`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, if we wanted to look at the average number of students who took each test, then we can take the number of overall test takers and divide that by the number of of unique exam names. I chose to use exam names, rather than base_exam_ids, as there was no way to know if base_exam_ids were completely unique for each course.\n",
    "\n",
    "From this methodology, I was able to calculate that the average exam had 31.8 students take it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`spark.sql(\"select count(*)/count(distinct exam_name) as MeanTakers from extracted\").show() `\n",
    "\n",
    "`+------------------+\n",
    "|        MeanTakers|\n",
    "+------------------+\n",
    "|31.844660194174757|\n",
    "+------------------+`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimately, from this analysis, we were able to find a few key pieces of information.\n",
    "\n",
    "* We found that there are 3,280 assessments in the dataset.\n",
    "* We found that the most popular courses are:\n",
    " * Learning Git (394 Students)\n",
    " * Introduction to Python (162 Students)\n",
    " * Introduction to Java 8 (158 Students)\n",
    "* We found that there were four courses that only one student took. The least common courses are:\n",
    " * Nulls, Three-valued Logic and Missing Information (1 Student)\n",
    " * Native Web Apps for Android (1 Student)\n",
    " * Learning to Visualize Data with D3.js (1 Student)\n",
    " * Operating Red Hat Enterprise Linux Servers (1 Student)\n",
    "* The introductory machine learning course appeared to have more students take it than the advanced machine learning course did. \n",
    " * If both classes were combined, it would be the second most popular course, indicating that further analysis could be done on the course subject matter.\n",
    "*  The average course had 31.8 students take it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Write df to parquet.\n",
    "`extracted.write.parquet(\"/tmp/extracted2\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### exit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`exit()`"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
